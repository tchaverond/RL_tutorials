# -*- coding: utf-8 -*-


import numpy as np
from tqdm import tqdm
from matplotlib import pyplot as plt

np.random.seed(0)



class CasinoMachine:

    def __init__(self, id, prob):
        self.id = id
        self.prob = prob

    def pull(self):
        return np.random.random() < self.prob


class GreedyAgent:

    def __init__(self, choices, epsilon):
        self.epsilon = epsilon
        # for each choice, list of (avg score, nb of explorations)
        self.scores = {choice:[0,0] for choice in choices}
        # output structures
        self.choices = []
        self.rewards = []

    def pick_choice(self):
        return np.random.choice(list(self.scores.keys()))

    def update_score(self, choice, reward):
        self.scores[choice][1] += 1
        times_chosen = self.scores[choice][1]
        self.scores[choice][0] = (1 - 1/times_chosen) * self.scores[choice][0] + reward/times_chosen

    def run(self, max_iterations):
        for _ in tqdm(range(max_iterations)):
            explore = np.random.random() < self.epsilon
            if explore:
                choice = self.pick_choice()
            else:
                choice = max(self.scores, key=self.scores.get)
            self.choices.append(choice.id)
            reward = choice.pull()
            self.rewards.append(reward)
            self.update_score(choice, reward)




# initialization
bandit_count = 10
bandits = [CasinoMachine(id=i, prob=np.random.random()) for i in range(bandit_count)]

# TODO: make epsilon decrease with time (linear/cubic scheme?)
agent = GreedyAgent(bandits, epsilon=0.1)
agent.run(max_iterations=1000)

# observed vs actual scores
observed = [(k.id, v[0]) for k,v in agent.scores.items()]
reality = [(k.id, k.prob) for k in agent.scores.keys()]
plt.figure()
plt.scatter([i[0] for i in observed], [i[1] for i in observed])
plt.scatter([i[0] for i in reality], [i[1] for i in reality])
plt.legend(('observed', 'reality'))
plt.title("Observed vs actual winning probabilities of bandit matchines")
plt.show()

# choices over time
fig = plt.figure()
observed = dict(observed)
scat = plt.scatter(np.arange(len(agent.choices)), agent.choices, c=[observed[c] for c in agent.choices])
plt.clim(0,1)
fig.colorbar(scat)
plt.title("Chosen machine over time (with their final observed score)")
plt.show()

# avg reward over time
plt.figure()
plt.plot(np.arange(len(agent.rewards)), np.cumsum(agent.rewards) / (1+np.arange(len(agent.rewards))))
plt.title("Evolution of rewards over time")
plt.xscale('log')
plt.show()

# epsilon comparison
agents = [GreedyAgent(bandits, epsilon=e) for e in [.01, .05, .1]]
for agent in agents:
    agent.run(max_iterations=100000)
# avg reward over time
plt.figure()
for agent in agents:
    plt.plot(np.arange(len(agent.rewards)), np.cumsum(agent.rewards) / (1+np.arange(len(agent.rewards))))
plt.title("Evolution of rewards over time")
plt.xscale('log')
plt.legend(['epsilon = ' + str(agent.epsilon) for agent in agents])
plt.show()
