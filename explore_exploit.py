# -*- coding: utf-8 -*-


import numpy as np
from tqdm import tqdm
from matplotlib import pyplot as plt
from scipy import stats as scs

np.random.seed(0)



class CasinoMachine:

    def __init__(self, id, prob):
        self.id = id
        self.prob = prob

    def pull(self):
        return np.random.random() < self.prob


# TODO: refactor this into standard agent and move GreedyAgent separately
class GreedyAgent:

    def __init__(self, choices, epsilon=.0, decrease_scheme=None, optimistic=False):
        self.epsilon = epsilon
        self.decrease_scheme = decrease_scheme
        self.is_optimistic = optimistic
        initial_score = 1 if optimistic else 0
        # for each choice, list of (avg score, nb of explorations)
        self.scores = {choice:[initial_score, 0] for choice in choices}
        # output structures
        self.choices = []
        self.rewards = []

    def pick_choice(self):
        return np.random.choice(list(self.scores.keys()))

    def pick_best(self):
        return max(self.scores, key=self.scores.get)

    def update_score(self, choice, reward):
        self.scores[choice][1] += 1
        times_chosen = self.scores[choice][1]
        self.scores[choice][0] = (1 - 1/times_chosen) * self.scores[choice][0] + reward/times_chosen

    def update_epsilon(self, t):
        if self.decrease_scheme == 'linear':
            self.epsilon = 1 / t
        elif self.decrease_scheme == 'cubic':
            self.epsilon = 1 / t**3
        elif self.decrease_scheme == 'log':
            if t != 1:
                self.epsilon = 1 / np.log(t)

    def run(self, max_iterations):
        for t in tqdm(range(max_iterations)):
            explore = np.random.random() < self.epsilon
            if explore:
                choice = self.pick_choice()
            else:
                choice = self.pick_best()
            self.choices.append(choice.id)
            reward = choice.pull()
            self.rewards.append(reward)
            self.update_score(choice, reward)
            if self.decrease_scheme is not None and t!= 0:
                self.update_epsilon(t)


# TODO: may need to be updated for dealing with probabilities instead of real numbers as rewards
# just like I use a Beta distribution instead of a Gaussian one for the bayesian agent
class UCBAgent(GreedyAgent):

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def pick_best(self):
        ucbs = {}
        total_iter = len(self.choices)
        for choice,score in self.scores.items():
            if score[1] == 0:
                ucbs[choice] = np.inf
            else:
                ucbs[choice] = score[0] + np.sqrt(2*np.log(total_iter) / score[1])
        return max(ucbs, key=ucbs.get)


class BayesianAgent(GreedyAgent):

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def pick_best(self):
        estimates = {}
        for choice,score in self.scores.items():
            success_count = int(score[1] * score[0])
            a = 1 + success_count
            b = 1 + score[1] - success_count
            estimates[choice] = scs.beta.rvs(a,b)
        return max(estimates, key=estimates.get)




# initialization
bandit_count = 10
bandits = [CasinoMachine(id=i, prob=np.random.random()) for i in range(bandit_count)]

agent = GreedyAgent(bandits, epsilon=.1)
agent.run(max_iterations=1000)

# Agent using Upper Confidence Bound
agent = UCBAgent(bandits)
agent.run(max_iterations=100000)

# Agent using Bayesian Sampling
agent = BayesianAgent(bandits)
agent.run(max_iterations=100000)

# observed vs actual scores
observed = [(k.id, v[0]) for k,v in agent.scores.items()]
reality = [(k.id, k.prob) for k in agent.scores.keys()]
plt.figure()
plt.scatter([i[0] for i in observed], [i[1] for i in observed])
plt.scatter([i[0] for i in reality], [i[1] for i in reality])
plt.legend(('observed', 'reality'))
plt.title("Observed vs actual winning probabilities of bandit matchines")
plt.show()

# choices over time
fig = plt.figure()
observed = dict(observed)
scat = plt.scatter(np.arange(len(agent.choices)), agent.choices, c=[observed[c] for c in agent.choices], s=4)
plt.clim(0,1)
fig.colorbar(scat)
plt.title("Chosen machine over time (with their final observed score)")
plt.show()

# avg reward over time
plt.figure()
plt.plot(np.arange(len(agent.rewards)), np.cumsum(agent.rewards) / (1+np.arange(len(agent.rewards))))
plt.title("Evolution of rewards over time")
plt.xscale('log')
plt.show()

# epsilon comparison
agents = [GreedyAgent(bandits, epsilon=e, optimistic=o)
          for e,o in zip([0,.01,.01,.05,.05,.1,.1], [True,False,True,False,True,False,True])]
for agent in agents:
    agent.run(max_iterations=100000)
plt_legend = ["epsilon = " + str(agent.epsilon) + "; optimistic = " + str(agent.is_optimistic) for agent in agents]

# epsilon-greedy vs optimistic initial value vs ucb1 vs bayesian sampling
agents = [GreedyAgent(bandits, epsilon=1, decrease_scheme='log'), GreedyAgent(bandits, optimistic=True),
          UCBAgent(bandits), BayesianAgent(bandits)]
for agent in agents:
    agent.run(max_iterations=int(10e5))
plt_legend = ["greedy (linear, eps=0.1)", "optimistic", "ucb1", "bayesian sampling"]

# avg reward over time
plt.figure()
for agent in agents:
    plt.plot(np.arange(len(agent.rewards)), np.cumsum(agent.rewards) / (1+np.arange(len(agent.rewards))))
plt.title("Evolution of rewards over time")
plt.xscale('log')
plt.legend(plt_legend)
plt.show()
